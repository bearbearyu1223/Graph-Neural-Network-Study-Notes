{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjg6WAVAWew0nICqWgngYW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bearbearyu1223/Graph-Neural-Network-Study-Notes/blob/main/graph_embedding_and_graph_isomorphism_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Isomorphism Network (GIN)"
      ],
      "metadata": {
        "id": "8v0xRZAtOtjq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkT5i_KZLztO",
        "outputId": "c9641e89-3718-41f8-adbc-284b54582550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries \n",
        "\n",
        "import torch\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 300\n",
        "plt.rcParams.update({'font.size': 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Proteins Dataset \n",
        "\n",
        "[PROTEINS](https://chrsmrrs.github.io/datasets/docs/datasets/) is a popular \n",
        "dataset in bioinformatics. It is a collection of 1113 graphs representing proteins, where nodes are amino acids. Two nodes are connected by an edge when they are close enough (< 0.6 nanometers). The goal is to classify each protein as an **enzyme** or not. \n",
        "\n",
        "Enzymes are a particular type of proteins that act as catalysts to speed up chemical reactions in the cell. They are essential for digestion (e.g., lipases), respiration (e.g., oxidases), and other crucial functions of the human body. They are also used in commercial applications, like the production of antibiotics."
      ],
      "metadata": {
        "id": "CnX9GZcMOrsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root=\".\", name=\"PROTEINS\").shuffle()\n",
        "\n",
        "print(f'Dataset: {dataset}')\n",
        "print('-------------------')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of nodes: {dataset[0].x.shape[0]}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhiiBr7uOzEj",
        "outputId": "42b30826-fc4c-4422-c2c2-d0366f6851f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: PROTEINS(1113)\n",
            "-------------------\n",
            "Number of graphs: 1113\n",
            "Number of nodes: 20\n",
            "Number of features: 3\n",
            "Number of classes: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Create min-batches of the Dataset"
      ],
      "metadata": {
        "id": "19t8UKt8TcuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_dataset = dataset[:int(len(dataset)*0.8)]\n",
        "val_dataset   = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)]\n",
        "test_dataset  = dataset[int(len(dataset)*0.9):]\n",
        "\n",
        "print(f'Training set   = {len(train_dataset)} graphs')\n",
        "print(f'Validation set = {len(val_dataset)} graphs')\n",
        "print(f'Test set       = {len(test_dataset)} graphs')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "print('\\nTrain loader:')\n",
        "for i, subgraph in enumerate(train_loader):\n",
        "    print(f'  Subgraph {i}: {subgraph}')\n",
        "\n",
        "print('\\nValidation loader:')\n",
        "for i, subgraph in enumerate(val_loader):\n",
        "    print(f'  Subgraph {i}: {subgraph}')\n",
        "\n",
        "print('\\nTest loader:')\n",
        "for i, subgraph in enumerate(test_loader):\n",
        "    print(f'  Subgraph {i}: {subgraph}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zALI40ZTLRl",
        "outputId": "f64148bd-5bdd-4272-f78a-a2da2fbf2be5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set   = 890 graphs\n",
            "Validation set = 111 graphs\n",
            "Test set       = 112 graphs\n",
            "\n",
            "Train loader:\n",
            "  Subgraph 0: DataBatch(edge_index=[2, 6524], x=[1772, 3], y=[64], batch=[1772], ptr=[65])\n",
            "  Subgraph 1: DataBatch(edge_index=[2, 13370], x=[3614, 3], y=[64], batch=[3614], ptr=[65])\n",
            "  Subgraph 2: DataBatch(edge_index=[2, 9164], x=[2517, 3], y=[64], batch=[2517], ptr=[65])\n",
            "  Subgraph 3: DataBatch(edge_index=[2, 8978], x=[2381, 3], y=[64], batch=[2381], ptr=[65])\n",
            "  Subgraph 4: DataBatch(edge_index=[2, 9876], x=[2589, 3], y=[64], batch=[2589], ptr=[65])\n",
            "  Subgraph 5: DataBatch(edge_index=[2, 10626], x=[2682, 3], y=[64], batch=[2682], ptr=[65])\n",
            "  Subgraph 6: DataBatch(edge_index=[2, 6858], x=[1871, 3], y=[64], batch=[1871], ptr=[65])\n",
            "  Subgraph 7: DataBatch(edge_index=[2, 9620], x=[2585, 3], y=[64], batch=[2585], ptr=[65])\n",
            "  Subgraph 8: DataBatch(edge_index=[2, 8710], x=[2330, 3], y=[64], batch=[2330], ptr=[65])\n",
            "  Subgraph 9: DataBatch(edge_index=[2, 8630], x=[2356, 3], y=[64], batch=[2356], ptr=[65])\n",
            "  Subgraph 10: DataBatch(edge_index=[2, 7788], x=[2070, 3], y=[64], batch=[2070], ptr=[65])\n",
            "  Subgraph 11: DataBatch(edge_index=[2, 10714], x=[2919, 3], y=[64], batch=[2919], ptr=[65])\n",
            "  Subgraph 12: DataBatch(edge_index=[2, 9162], x=[2448, 3], y=[64], batch=[2448], ptr=[65])\n",
            "  Subgraph 13: DataBatch(edge_index=[2, 7960], x=[2104, 3], y=[58], batch=[2104], ptr=[59])\n",
            "\n",
            "Validation loader:\n",
            "  Subgraph 0: DataBatch(edge_index=[2, 9242], x=[2416, 3], y=[64], batch=[2416], ptr=[65])\n",
            "  Subgraph 1: DataBatch(edge_index=[2, 7432], x=[2053, 3], y=[47], batch=[2053], ptr=[48])\n",
            "\n",
            "Test loader:\n",
            "  Subgraph 0: DataBatch(edge_index=[2, 10450], x=[2855, 3], y=[64], batch=[2855], ptr=[65])\n",
            "  Subgraph 1: DataBatch(edge_index=[2, 6984], x=[1909, 3], y=[48], batch=[1909], ptr=[49])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Representational Power of Graph Isomorphism Network (GIN)\n",
        "[GIN](https://arxiv.org/abs/1810.00826v3) was designed to maximize the **representational power** of a GNN. \n",
        "\n",
        "### 3.1 Weisfeiler-Lehman Test\n",
        "A way to characterize the \"representational power\" of a GNN is to use the  [Weisfeiler-Lehman graph isomorphism test](https://davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/). [Isomorphism Graphs](https://en.wikipedia.org/wiki/Graph_isomorphism) mean the graphs have the same structure: identical connections but a permutation of nodes\n",
        "\n",
        "One should note that the Weisfeiler-Lehman(WL) test is able to tell if two graphs **are non-isomorphic**, but it cannot guarantee that they are isomorphic.\n",
        "\n",
        "In the WL test:\n",
        "\n",
        "\n",
        "1. Every node starts with **the same label**.\n",
        "2. Labels from neighboring nodes are **aggregated and** **hashed** to produce a new label.\n",
        "3. The previous step is **repeated** until the labels stop changing.\n",
        "\n",
        "### 3.2 One Aggregator to Rule Them All\n",
        "To be as good as the WL test, the aggregator which is designed to aggregate the feature vectors from the neighboring nodes must produce different node embeddings when dealing with non-isomorphic graphs. How do we design this aggregator? **We just learn them with Multi-Layer Perceptron(MLP)**.  \n",
        "\n",
        "$\\displaystyle h_i = MLP\\big((1+ϵ)⋅x_i+\\sum_{j\\in N_i}x_j\\big)$\n",
        "\n",
        "Where $ϵ$ is a learnable parameter represents the importance of the target node as compared to its neighbors. \n",
        "\n",
        "### 3.3 Global Pooling \n",
        "Global Pooling or Graph-Level Readout consists of producing a graph embedding using the node embeddings calculated by the GNN. \n",
        "\n",
        "A simple way to obtain a graph embedding $h_G$ is to use the $mean$, $sum$, or $max$ of every node embedding $h_i$. For **Global Pooling**, embeddings of nodes from each layer are summed and the result is concantenated. \n",
        "\n",
        "$\\displaystyle h_G = \\sum_{i=0}^{N}h^0_i ||…|| \\sum_{i=0}^{N}h^k_i$\n",
        "\n"
      ],
      "metadata": {
        "id": "rB1iqLWMUWbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Benchmark the performance of GCN vs GIN"
      ],
      "metadata": {
        "id": "k1k24C69dQCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU\n",
        "from torch_geometric.nn import GINConv, GCNConv\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool "
      ],
      "metadata": {
        "id": "nFJFBr32UQso"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                      lr=0.01,\n",
        "                                      weight_decay=0.01)\n",
        "    epochs = 100\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs+1):\n",
        "        total_loss = 0\n",
        "        acc = 0\n",
        "        val_loss = 0\n",
        "        val_acc = 0\n",
        "\n",
        "        # Train on batches\n",
        "        for data in loader:\n",
        "          optimizer.zero_grad()\n",
        "          _, out = model(data.x, data.edge_index, data.batch)\n",
        "          loss = criterion(out, data.y)\n",
        "          total_loss += loss / len(loader)\n",
        "          acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Validation\n",
        "          val_loss, val_acc = test(model, val_loader)\n",
        "        if(epoch % 10 == 0):\n",
        "          print(f'Epoch {epoch:>3} '\n",
        "                f'| Train Loss: {total_loss:.2f} '\n",
        "                f'| Train Acc: {acc*100:>5.2f}% '\n",
        "                f'| Val Loss: {val_loss:.2f} '\n",
        "                f'| Val Acc: {val_acc*100:.2f}%')\n",
        "          \n",
        "    test_loss, test_acc = test(model, test_loader)\n",
        "    print(f'Test Loss: {test_loss:.2f} | Test Acc: {test_acc*100:.2f}%')\n",
        "    \n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, loader):\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    for data in loader:\n",
        "        _, out = model(data.x, data.edge_index, data.batch)\n",
        "        loss += criterion(out, data.y) / len(loader)\n",
        "        acc += accuracy(out.argmax(dim=1), data.y) / len(loader)\n",
        "\n",
        "    return loss, acc\n",
        "\n",
        "def accuracy(pred_y, y):\n",
        "    \"\"\"Calculate accuracy.\"\"\"\n",
        "    return ((pred_y == y).sum() / len(y)).item()"
      ],
      "metadata": {
        "id": "QeN54-c2NyET"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Classify Graphs via GCN "
      ],
      "metadata": {
        "id": "iQy2Lnm1N3aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, dim_in, dim_h, dim_out):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(dim_in, dim_h)\n",
        "        self.conv2 = GCNConv(dim_h, dim_h)\n",
        "        self.conv3 = GCNConv(dim_h, dim_h)\n",
        "        self.lin = Linear(dim_h, dim_out)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # Node Embedding \n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = h.relu()\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = h.relu()\n",
        "        h = self.conv3(h, edge_index)\n",
        "        \n",
        "        # Graph Embedding \n",
        "        hG = global_mean_pool(h, batch)\n",
        "\n",
        "        # Classifier\n",
        "        hG = F.dropout(hG, p=0.5, training=self.training)\n",
        "        hG = self.lin(hG)\n",
        "        \n",
        "        return hG, F.log_softmax(hG, dim=1)"
      ],
      "metadata": {
        "id": "gRkFtowsN8bS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "gcn = GCN(dim_in=dataset.num_features, dim_h=32, dim_out=dataset.num_classes)\n",
        "gcn = train(gcn, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUxZ7wFxOp9S",
        "outputId": "0127f498-3372-448c-95fa-b83f70b37e44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 0.68 | Train Acc: 59.62% | Val Loss: 0.68 | Val Acc: 58.69%\n",
            "Epoch  10 | Train Loss: 0.68 | Train Acc: 59.36% | Val Loss: 0.68 | Val Acc: 59.26%\n",
            "Epoch  20 | Train Loss: 0.68 | Train Acc: 59.27% | Val Loss: 0.67 | Val Acc: 59.82%\n",
            "Epoch  30 | Train Loss: 0.68 | Train Acc: 59.33% | Val Loss: 0.68 | Val Acc: 58.13%\n",
            "Epoch  40 | Train Loss: 0.68 | Train Acc: 59.37% | Val Loss: 0.67 | Val Acc: 60.11%\n",
            "Epoch  50 | Train Loss: 0.68 | Train Acc: 59.33% | Val Loss: 0.67 | Val Acc: 60.11%\n",
            "Epoch  60 | Train Loss: 0.68 | Train Acc: 59.37% | Val Loss: 0.68 | Val Acc: 58.98%\n",
            "Epoch  70 | Train Loss: 0.68 | Train Acc: 59.39% | Val Loss: 0.68 | Val Acc: 58.69%\n",
            "Epoch  80 | Train Loss: 0.68 | Train Acc: 59.33% | Val Loss: 0.68 | Val Acc: 58.98%\n",
            "Epoch  90 | Train Loss: 0.68 | Train Acc: 59.39% | Val Loss: 0.68 | Val Acc: 57.28%\n",
            "Epoch 100 | Train Loss: 0.68 | Train Acc: 59.30% | Val Loss: 0.68 | Val Acc: 57.56%\n",
            "Test Loss: 0.66 | Test Acc: 62.50%\n",
            "CPU times: user 1min 33s, sys: 775 ms, total: 1min 34s\n",
            "Wall time: 1min 41s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Classify Graphs via GIN  "
      ],
      "metadata": {
        "id": "LjoLur3zNexc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GIN(torch.nn.Module): \n",
        "  def __init__(self, dim_in, dim_h, dim_out):\n",
        "    super().__init__()\n",
        "    self.conv1 = GINConv(\n",
        "        Sequential(\n",
        "            Linear(dim_in, dim_h),\n",
        "            BatchNorm1d(dim_h), \n",
        "            ReLU(), \n",
        "            Linear(dim_h, dim_h), \n",
        "            ReLU()\n",
        "            ))\n",
        "    self.conv2 = GINConv(\n",
        "        Sequential(\n",
        "            Linear(dim_h, dim_h),\n",
        "            BatchNorm1d(dim_h), \n",
        "            ReLU(), \n",
        "            Linear(dim_h, dim_h), \n",
        "            ReLU()\n",
        "            ))\n",
        "    self.conv3 = GINConv(\n",
        "        Sequential(\n",
        "            Linear(dim_h, dim_h),\n",
        "            BatchNorm1d(dim_h), \n",
        "            ReLU(), \n",
        "            Linear(dim_h, dim_h), \n",
        "            ReLU()\n",
        "            ))\n",
        "    self.lin1 = Linear(dim_h * 3, dim_h *3)\n",
        "    self.lin2 = Linear(dim_h * 3, dim_out)\n",
        "  \n",
        "  def forward(self, x, edge_index, batch):\n",
        "    h1 = self.conv1(x, edge_index)\n",
        "    h2 = self.conv2(h1, edge_index)\n",
        "    h3 = self.conv3(h2, edge_index)\n",
        "\n",
        "    h1 = global_add_pool(h1, batch)\n",
        "    h2 = global_add_pool(h2, batch)\n",
        "    h3 = global_add_pool(h3, batch)\n",
        "\n",
        "    h = torch.cat((h1, h2, h3), dim=1)\n",
        "\n",
        "    h = self.lin1(h)\n",
        "    h = F.relu(h)\n",
        "    h = F.dropout(h, p=0.5, training=self.training)\n",
        "    h = self.lin2(h)\n",
        "\n",
        "    return h, F.log_softmax(h, dim=1)    "
      ],
      "metadata": {
        "id": "xYlo0KWWd2WF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "gin = GIN(dim_in=dataset.num_features, dim_h=32, dim_out=dataset.num_classes)\n",
        "gin = train(gin, train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEpBGcRLKpqf",
        "outputId": "a0e32504-a495-4ace-fc0e-3a4b35057f29"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 1.55 | Train Acc: 61.43% | Val Loss: 0.60 | Val Acc: 64.23%\n",
            "Epoch  10 | Train Loss: 0.56 | Train Acc: 75.38% | Val Loss: 0.57 | Val Acc: 70.20%\n",
            "Epoch  20 | Train Loss: 0.56 | Train Acc: 73.33% | Val Loss: 0.56 | Val Acc: 73.74%\n",
            "Epoch  30 | Train Loss: 0.54 | Train Acc: 74.02% | Val Loss: 0.56 | Val Acc: 70.89%\n",
            "Epoch  40 | Train Loss: 0.55 | Train Acc: 74.09% | Val Loss: 0.55 | Val Acc: 71.26%\n",
            "Epoch  50 | Train Loss: 0.53 | Train Acc: 74.37% | Val Loss: 0.55 | Val Acc: 72.96%\n",
            "Epoch  60 | Train Loss: 0.53 | Train Acc: 75.44% | Val Loss: 0.55 | Val Acc: 72.39%\n",
            "Epoch  70 | Train Loss: 0.52 | Train Acc: 76.05% | Val Loss: 0.56 | Val Acc: 71.54%\n",
            "Epoch  80 | Train Loss: 0.53 | Train Acc: 74.88% | Val Loss: 0.57 | Val Acc: 71.54%\n",
            "Epoch  90 | Train Loss: 0.52 | Train Acc: 75.02% | Val Loss: 0.56 | Val Acc: 71.26%\n",
            "Epoch 100 | Train Loss: 0.53 | Train Acc: 74.33% | Val Loss: 0.55 | Val Acc: 74.02%\n",
            "Test Loss: 0.40 | Test Acc: 82.81%\n",
            "CPU times: user 1min 1s, sys: 265 ms, total: 1min 1s\n",
            "Wall time: 1min 1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Ensemble learning of GIN and GCN\n",
        "We can achieve better prediction performance by combing the predictions from multiple models. The simplest approach is to take the mean of the normalized output vectors. "
      ],
      "metadata": {
        "id": "Kq-DfR-rQIqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gcn.eval()\n",
        "gin.eval()\n",
        "acc_gcn = 0\n",
        "acc_gin = 0\n",
        "acc = 0\n",
        "\n",
        "for data in test_loader:\n",
        "    _, out_gcn = gcn(data.x, data.edge_index, data.batch)\n",
        "    _, out_gin = gin(data.x, data.edge_index, data.batch)\n",
        "    out = (out_gcn + out_gin)/2\n",
        "\n",
        "    acc_gcn += accuracy(out_gcn.argmax(dim=1), data.y) / len(test_loader)\n",
        "    acc_gin += accuracy(out_gin.argmax(dim=1), data.y) / len(test_loader)\n",
        "    acc += accuracy(out.argmax(dim=1), data.y) / len(test_loader)\n",
        "\n",
        "# Print results\n",
        "print(f'GCN accuracy:     {acc_gcn*100:.2f}%')\n",
        "print(f'GIN accuracy:     {acc_gin*100:.2f}%')\n",
        "print(f'GCN+GIN accuracy: {acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMi-ntlDQDv3",
        "outputId": "f551cf0c-b777-476e-8698-033b4ee40b78"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN accuracy:     62.50%\n",
            "GIN accuracy:     82.81%\n",
            "GCN+GIN accuracy: 80.99%\n"
          ]
        }
      ]
    }
  ]
}